{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Text Normalization and Entity Verification for GPT Fine-Tuning</h1>"
      ],
      "metadata": {
        "id": "wsK0tDnOEuAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    clean_text = ' '.join(filtered_tokens)\n",
        "\n",
        "    return clean_text, entities\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "processed_text, detected_entities = preprocess_text(text)\n",
        "print(\"Processed Text:\", processed_text)\n",
        "print(\"Detected Entities:\", detected_entities)"
      ],
      "metadata": {
        "id": "dOEeoxa1Fgaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Validation of Training Data Relevance</h1>"
      ],
      "metadata": {
        "id": "XrdbItU_Fku-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "documents = [\n",
        "    \"How to reset my router?\",\n",
        "    \"Can you help me upgrade my plan?\",\n",
        "    \"Troubleshooting network issues\",\n",
        "    \"What are the latest offers?\",\n",
        "    \"Billing question regarding overcharges\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "new_doc = [\"Network speed is slow, how can I improve it?\"]\n",
        "new_doc_vector = vectorizer.transform(new_doc)\n",
        "cosine_similarities = cosine_similarity(new_doc_vector, tfidf_matrix)\n",
        "print(\"Cosine Similarities:\\n\", cosine_similarities)"
      ],
      "metadata": {
        "id": "jvNj7ucQFio1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Data Cleansing and Anomaly Detection</h1>"
      ],
      "metadata": {
        "id": "vYPIM-5qE-u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {'Time': [\"2021-06-01 12:01:01\", \"2021-06-01 12:05:30\", \"2021-06-01 12:30:05\", \"outlier\", \"2021-06-01 12:45:10\"],\n",
        "        'Interaction_length': [300, 180, 450, 9999, 230]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['Time'] = pd.to_datetime(df['Time'], errors='coerce')\n",
        "df = df.dropna(subset=['Time'])\n",
        "\n",
        "z_scores = np.abs((df['Interaction_length'] - df['Interaction_length'].mean()) / df['Interaction_length'].std())\n",
        "df = df[z_scores < 3]\n",
        "\n",
        "print(\"Cleaned Data:\\n\", df)"
      ],
      "metadata": {
        "id": "NjRq0JQcFE8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Structuring for GPT-3.5: Role-Based Conversation Data</h1>"
      ],
      "metadata": {
        "id": "izxg3n2k8AzD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoF4Ln-u7_KD"
      },
      "outputs": [],
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an investment advice assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Is it a good time to invest in stocks?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"It depends on the market conditions and your personal financial goals. It's often wise to consult with a financial advisor.\"}\n",
        "]\n",
        "\n",
        "# Save to a JSON file with proper formatting\n",
        "with open('formatted_data.json', 'w') as file:\n",
        "    json.dump({\"messages\": conversation}, file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Splitting Data: Training and Validation Sets</h1>"
      ],
      "metadata": {
        "id": "voJu1Ymh8Jpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    data, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "5z-dAk_G8HWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Serialization to JSONL for Training</h1>"
      ],
      "metadata": {
        "id": "fFvv6Cs98VuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def serialize_to_jsonl(data_entries):\n",
        "    with open('output.jsonl', 'w') as file:\n",
        "        for entry in data_entries:\n",
        "            json_object = json.dumps({\n",
        "                'prompt': entry['question'],\n",
        "                'completion': entry['answer']\n",
        "            })\n",
        "            file.write(json_object + '\\n')\n",
        "\n",
        "# Example entries\n",
        "data_entries = [\n",
        "    {'question': 'What is the capital of France?', 'answer': 'Paris'},\n",
        "    {'question': 'What is the largest planet in our solar system?', 'answer': 'Jupiter'}\n",
        "]\n",
        "serialize_to_jsonl(data_entries)"
      ],
      "metadata": {
        "id": "14N7DFql8cfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Uploading Data for Fine-Tuning with OpenAI</h1>"
      ],
      "metadata": {
        "id": "obX9HPTH8fkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"your api key\")\n",
        "\n",
        "# Assuming you've already installed the OpenAI library and set your API key\n",
        "response = client.files.create(\n",
        "    file=open(\"training_data.jsonl\", \"rb\"),\n",
        "    purpose='fine-tune'\n",
        ")\n",
        "\n",
        "print(\"Uploaded file ID:\", response['id'])"
      ],
      "metadata": {
        "id": "4R3LOUJz8mcl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}